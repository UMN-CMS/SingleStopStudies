@misc{frate_modeling_2017,
  title = {Modeling {{Smooth Backgrounds}} and {{Generic Localized Signals}} with {{Gaussian Processes}}},
  year = {2017},
  month = sep,
  number = {arXiv:1709.05681},
  eprint = {1709.05681},
  primaryclass = {hep-ex, physics:hep-ph, physics:physics},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1709.05681},
  urldate = {2023-08-29},
  abstract = {We describe a procedure for constructing a model of a smooth data spectrum using Gaussian processes rather than the historical parametric description. This approach considers a fuller space of possible functions, is robust at increasing luminosity, and allows us to incorporate our understanding of the underlying physics. We demonstrate the application of this approach to modeling the background to searches for dijet resonances at the Large Hadron Collider and describe how the approach can be used in the search for generic localized signals.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {{Physics - Data Analysis, Statistics and Probability},High Energy Physics - Experiment,High Energy Physics - Phenomenology},
  file = {/Users/CharlieKapsiak/Google Drive/Zotero/Frate et al_2017_Modeling Smooth Backgrounds and Generic Localized Signals with Gaussian.pdf},
  author = {Frate, Meghan and {Et al.}}
}

@article{gandrakota_model_2023,
  title = {Model Selection and Signal Extraction Using {{Gaussian Process}} Regression},
  author = {Gandrakota, Abhijith and Lath, Amit and Morozov, Alexandre V. and Murthy, Sindhu},
  year = {2023},
  month = feb,
  journal = {Journal of High Energy Physics},
  volume = {2023},
  number = {2},
  pages = {230},
  issn = {1029-8479},
  doi = {10.1007/JHEP02(2023)230},
  url = {https://link.springer.com/10.1007/JHEP02(2023)230},
  urldate = {2023-09-11},
  abstract = {We present a novel computational approach for extracting localized signals from smooth background distributions. We focus on datasets that can be naturally presented as binned integer counts, demonstrating our procedure on the CERN open dataset with the Higgs boson signature, from the ATLAS collaboration at the Large Hadron Collider. Our approach is based on Gaussian Process (GP) regression --- a powerful and flexible machine learning technique which has allowed us to model the background without specifying its functional form explicitly and separately measure the background and signal contributions in a robust and reproducible manner. Unlike functional fits, our GPregression-based approach does not need to be constantly updated as more data becomes available. We discuss how to select the GP kernel type, considering trade-offs between kernel complexity and its ability to capture the features of the background distribution. We show that our GP framework can be used to detect the Higgs boson resonance in the data with more statistical significance than a polynomial fit specifically tailored to the dataset. Finally, we use Markov Chain Monte Carlo (MCMC) sampling to confirm the statistical significance of the extracted Higgs signature.},
  langid = {english},
  file = {/Users/CharlieKapsiak/Zotero/storage/B9NTEVD2/Gandrakota et al. - 2023 - Model selection and signal extraction using Gaussi.pdf}
}

@misc{gardner_gpytorch_2021,
  title = {{{GPyTorch}}: {{Blackbox Matrix-Matrix Gaussian Process Inference}} with {{GPU Acceleration}}},
  shorttitle = {{{GPyTorch}}},
  year = {2021},
  month = jun,
  number = {arXiv:1809.11165},
  eprint = {1809.11165},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1809.11165},
  urldate = {2024-05-03},
  abstract = {Despite advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on developments in computing hardware. We present an efficient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. BBMM reduces the asymptotic complexity of exact GP inference from O(n3) to O(n2). Adapting this algorithm to scalable approximations and complex GP models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM uses a specialized preconditioner to substantially speed up convergence. In experiments we show that BBMM effectively uses GPU hardware to dramatically accelerate both exact GP inference and scalable approximations. Additionally, we provide GPyTorch, a software platform for scalable GP inference via BBMM, built on PyTorch.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/CharlieKapsiak/Google Drive/Zotero/Gardner et al_2021_GPyTorch.pdf},
  author = {Gardner, Jacob R. and {Et al.}}
}

@book{gelman_bayesian_nodate,
  title = {Bayesian {{Data Analysis Third}} Edition (with Errors Fixed as of 15 {{February}} 2021)},
  langid = {english},
  file = {/Users/CharlieKapsiak/Google Drive/Zotero/Gelman et al_Bayesian Data Analysis Third edition (with errors Ô¨Åxed as of 15 February 2021).pdf},
  author = {Gelman, Andrew and {Et al.}}
}

@book{hahs-vaughn_statistical_nodate,
  title = {Statistical {{Concepts}} - a {{Second Course}}},
  author = {{Hahs-Vaughn}, Debbie L and Lomax, Richard G},
  langid = {english},
  file = {/Users/CharlieKapsiak/Google Drive/Zotero/Hahs-Vaughn_Lomax_Statistical Concepts - a Second Course.pdf}
}

@book{neal_mcmc_2011,
  title = {{{MCMC}} Using {{Hamiltonian}} Dynamics},
  author = {Neal, Radford M.},
  year = {2011},
  month = may,
  eprint = {1206.1901},
  primaryclass = {physics, stat},
  doi = {10.1201/b10905},
  url = {http://arxiv.org/abs/1206.1901},
  urldate = {2024-03-27},
  abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard-to-compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, I discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for deciding on acceptance or rejection, computing trajectories using fast approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories from taking much computation time.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Physics - Computational Physics,Statistics - Computation},
  file = {/Users/CharlieKapsiak/Zotero/storage/U8UGAXGM/Neal - 2011 - MCMC using Hamiltonian dynamics.pdf}
}

@book{rasmussen_gaussian_2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2006},
  series = {Adaptive Computation and Machine Learning},
  publisher = {MIT Press},
  address = {Cambridge, Mass},
  isbn = {978-0-262-18253-9},
  langid = {english},
  lccn = {QA274.4 .R37 2006},
  keywords = {Data processing,Gaussian processes,Machine learning,Mathematical models},
  annotation = {OCLC: ocm61285753},
  file = {/Users/CharlieKapsiak/Google Drive/Zotero/Rasmussen_Williams_2006_Gaussian processes for machine learning.pdf}
}

@misc{salimbeni_doubly_2017,
  title = {Doubly {{Stochastic Variational Inference}} for {{Deep Gaussian Processes}}},
  author = {Salimbeni, Hugh and Deisenroth, Marc},
  year = {2017},
  month = nov,
  number = {arXiv:1705.08933},
  eprint = {1705.08933},
  primaryclass = {stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1705.08933},
  urldate = {2024-05-03},
  abstract = {Gaussian processes (GPs) are a good choice for function approximation as they are flexible, robust to overfitting, and provide well-calibrated predictive uncertainty. Deep Gaussian processes (DGPs) are multi-layer generalizations of GPs, but inference in these models has proved challenging. Existing approaches to inference in DGP models assume approximate posteriors that force independence between the layers, and do not work well in practice. We present a doubly stochastic variational inference algorithm that does not force independence between layers. With our method of inference we demonstrate that a DGP model can be used effectively on data ranging in size from hundreds to a billion points. We provide strong empirical evidence that our inference scheme for DGPs works well in practice in both classification and regression.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Machine Learning},
  file = {/Users/CharlieKapsiak/Zotero/storage/MJZXX2YX/Salimbeni and Deisenroth - 2017 - Doubly Stochastic Variational Inference for Deep G.pdf}
}

@article{titsias_variational_nodate,
  title = {Variational {{Learning}} of {{Inducing Variables}} in {{Sparse Gaussian Processes}}},
  author = {Titsias, Michalis K},
  abstract = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs are defined to be variational parameters which are selected by minimizing the Kullback-Leibler divergence between the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.},
  langid = {english},
  file = {/Users/CharlieKapsiak/Google Drive/Zotero/Titsias_Variational Learning of Inducing Variables in Sparse Gaussian Processes.pdf}
}

@article{wainwright_graphical_2007-1,
  title = {Graphical {{Models}}, {{Exponential Families}}, and {{Variational Inference}}},
  author = {Wainwright, Martin J. and Jordan, Michael I.},
  year = {2007},
  journal = {Foundations and Trends{\textregistered} in Machine Learning},
  volume = {1},
  number = {1--2},
  pages = {1--305},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000001},
  url = {http://www.nowpublishers.com/article/Details/MAL-001},
  urldate = {2024-03-25},
  langid = {english},
  file = {/Users/CharlieKapsiak/Zotero/storage/DGDEUR2T/Wainwright and Jordan - 2007 - Graphical Models, Exponential Families, and Variat.pdf}
}

@misc{wilson_deep_2015,
  title = {Deep {{Kernel Learning}}},
  author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
  year = {2015},
  month = nov,
  number = {arXiv:1511.02222},
  eprint = {1511.02222},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1511.02222},
  urldate = {2024-05-03},
  abstract = {We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric flexibility of kernel methods. Specifically, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation. These closed-form kernels can be used as drop-in replacements for standard kernels, with benefits in expressive power and scalability. We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process. Inference and learning cost O(n) for n training points, and predictions cost O(1) per test point. On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with flexible kernel learning models, and stand-alone deep architectures.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/CharlieKapsiak/Zotero/storage/E4LG3SDL/Wilson et al. - 2015 - Deep Kernel Learning.pdf}
}
