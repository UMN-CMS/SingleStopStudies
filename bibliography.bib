@misc{cms_collaboration_searches_2024,
  title = {Searches for Pair-Produced Multijet Resonances Using Data Scouting in Proton-Proton Collisions at \${\textbackslash}sqrt\{s\}\$ = 13 {{TeV}}},
  author = {CMS Collaboration},
  year = {2024},
  month = apr,
  number = {arXiv:2404.02992},
  eprint = {2404.02992},
  primaryclass = {hep-ex},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2404.02992},
  urldate = {2024-05-15},
  abstract = {Searches for pair-produced multijet signatures using data corresponding to an integrated luminosity of 128 fb\${\textasciicircum}\{-1\}\$ of proton-proton collisions at \${\textbackslash}sqrt\{s\}\$ = 13 TeV are presented. A data scouting technique is employed to record events with low jet scalar transverse momentum sum values. The electroweak production of particles predicted in \$R\$-parity violating supersymmetric models is probed for the first time with fully hadronic final states. This is the first search for prompt hadronically decaying mass-degenerate higgsinos, and extends current exclusions on \$R\$-parity violating top squarks and gluinos.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {High Energy Physics - Experiment},
  file = {/Users/CharlieKapsiak/Google Drive/Zotero/CMS Collaboration_2024_Searches for pair-produced multijet resonances using data scouting in.pdf}
}

@article{evans_lhc_2013,
  title = {{{LHC Coverage}} of {{RPV MSSM}} with {{Light Stops}}},
  author = {Evans, Jared A. and Kats, Yevgeny},
  year = {2013},
  month = apr,
  journal = {Journal of High Energy Physics},
  volume = {2013},
  number = {4},
  eprint = {1209.0764},
  pages = {28},
  issn = {1029-8479},
  doi = {10.1007/JHEP04(2013)028},
  url = {http://arxiv.org/abs/1209.0764},
  urldate = {2021-06-14},
  abstract = {We examine the sensitivity of recent LHC searches to signatures of supersymmetry with R-parity violation (RPV). Motivated by naturalness of the Higgs potential, which would favor light third-generation squarks, and the stringent LHC bounds on spectra in which the gluino or first and second generation squarks are light, we focus on scenarios dominated by the pair production of light stops. We consider the various possible direct and cascade decays of the stop that involve the trilinear RPV operators. We find that in many cases, the existing searches exclude stops in the natural mass range and beyond. However, typically there is little or no sensitivity to cases dominated by UDD operators or LQD operators involving taus. We propose several ideas for searches which could address the existing gaps in experimental coverage of these signals.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {High Energy Physics - Experiment,High Energy Physics - Phenomenology},
  file = {/Users/CharlieKapsiak/pdf_library/Evans_Kats_2013_LHC Coverage of RPV MSSM with Light Stops.pdf}
}

@misc{frate_modeling_2017,
  title = {Modeling {{Smooth Backgrounds}} and {{Generic Localized Signals}} with {{Gaussian Processes}}},
  year = {2017},
  month = sep,
  number = {arXiv:1709.05681},
  eprint = {1709.05681},
  primaryclass = {hep-ex, physics:hep-ph, physics:physics},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1709.05681},
  urldate = {2023-08-29},
  abstract = {We describe a procedure for constructing a model of a smooth data spectrum using Gaussian processes rather than the historical parametric description. This approach considers a fuller space of possible functions, is robust at increasing luminosity, and allows us to incorporate our understanding of the underlying physics. We demonstrate the application of this approach to modeling the background to searches for dijet resonances at the Large Hadron Collider and describe how the approach can be used in the search for generic localized signals.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {{Physics - Data Analysis, Statistics and Probability},High Energy Physics - Experiment,High Energy Physics - Phenomenology},
  file = {/Users/CharlieKapsiak/Google Drive/Zotero/Frate et al_2017_Modeling Smooth Backgrounds and Generic Localized Signals with Gaussian.pdf},
  author = {Frate, Meghan and {Et al.}}
}

@article{gandrakota_model_2023,
  title = {Model Selection and Signal Extraction Using {{Gaussian Process}} Regression},
  author = {Gandrakota, Abhijith and Lath, Amit and Morozov, Alexandre V. and Murthy, Sindhu},
  year = {2023},
  month = feb,
  journal = {Journal of High Energy Physics},
  volume = {2023},
  number = {2},
  pages = {230},
  issn = {1029-8479},
  doi = {10.1007/JHEP02(2023)230},
  url = {https://link.springer.com/10.1007/JHEP02(2023)230},
  urldate = {2023-09-11},
  abstract = {We present a novel computational approach for extracting localized signals from smooth background distributions. We focus on datasets that can be naturally presented as binned integer counts, demonstrating our procedure on the CERN open dataset with the Higgs boson signature, from the ATLAS collaboration at the Large Hadron Collider. Our approach is based on Gaussian Process (GP) regression --- a powerful and flexible machine learning technique which has allowed us to model the background without specifying its functional form explicitly and separately measure the background and signal contributions in a robust and reproducible manner. Unlike functional fits, our GPregression-based approach does not need to be constantly updated as more data becomes available. We discuss how to select the GP kernel type, considering trade-offs between kernel complexity and its ability to capture the features of the background distribution. We show that our GP framework can be used to detect the Higgs boson resonance in the data with more statistical significance than a polynomial fit specifically tailored to the dataset. Finally, we use Markov Chain Monte Carlo (MCMC) sampling to confirm the statistical significance of the extracted Higgs signature.},
  langid = {english},
  file = {/Users/CharlieKapsiak/Google Drive/Zotero/Gandrakota et al_2023_Model selection and signal extraction using Gaussian Process regression.pdf}
}

@misc{gardner_gpytorch_2021,
  title = {{{GPyTorch}}: {{Blackbox Matrix-Matrix Gaussian Process Inference}} with {{GPU Acceleration}}},
  shorttitle = {{{GPyTorch}}},
  year = {2021},
  month = jun,
  number = {arXiv:1809.11165},
  eprint = {1809.11165},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1809.11165},
  urldate = {2024-05-03},
  abstract = {Despite advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on developments in computing hardware. We present an efficient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. BBMM reduces the asymptotic complexity of exact GP inference from O(n3) to O(n2). Adapting this algorithm to scalable approximations and complex GP models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM uses a specialized preconditioner to substantially speed up convergence. In experiments we show that BBMM effectively uses GPU hardware to dramatically accelerate both exact GP inference and scalable approximations. Additionally, we provide GPyTorch, a software platform for scalable GP inference via BBMM, built on PyTorch.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/CharlieKapsiak/Google Drive/Zotero/Gardner et al_2021_GPyTorch.pdf},
  author = {Gardner, Jacob R. and {Et al.}}
}

@book{gelman_bayesian_nodate,
  title = {Bayesian {{Data Analysis Third}} Edition (with Errors Fixed as of 15 {{February}} 2021)},
  langid = {english},
  file = {/Users/CharlieKapsiak/Google Drive/Zotero/Gelman et al_Bayesian Data Analysis Third edition (with errors Ô¨Åxed as of 15 February 2021).pdf},
  author = {Gelman, Andrew and {Et al.}}
}

@book{hahs-vaughn_statistical_nodate,
  title = {Statistical {{Concepts}} - a {{Second Course}}},
  author = {{Hahs-Vaughn}, Debbie L and Lomax, Richard G},
  langid = {english},
  file = {/Users/CharlieKapsiak/Google Drive/Zotero/Hahs-Vaughn_Lomax_Statistical Concepts - a Second Course.pdf}
}

@book{neal_mcmc_2011,
  title = {{{MCMC}} Using {{Hamiltonian}} Dynamics},
  author = {Neal, Radford M.},
  year = {2011},
  month = may,
  eprint = {1206.1901},
  primaryclass = {physics, stat},
  doi = {10.1201/b10905},
  url = {http://arxiv.org/abs/1206.1901},
  urldate = {2024-03-27},
  abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard-to-compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, I discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for deciding on acceptance or rejection, computing trajectories using fast approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories from taking much computation time.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Physics - Computational Physics,Statistics - Computation},
  file = {/Users/CharlieKapsiak/Google Drive/Zotero/Neal_2011_MCMC using Hamiltonian dynamics.pdf}
}

@article{noauthor_comparison_2024,
  title = {Comparison of {{Gaussian}} Process Software},
  year = {2024},
  month = may,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=Comparison_of_Gaussian_process_software&oldid=1222498256#Comparison_table},
  urldate = {2024-05-13},
  abstract = {This is a comparison of statistical analysis software that allows doing inference with Gaussian processes often using approximations. This article is written from the point of view of Bayesian statistics, which may use a terminology different from the one commonly used in kriging. The next section should clarify the mathematical/computational meaning of the information provided in the table independently of contextual terminology.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1222498256},
  file = {/Users/CharlieKapsiak/Zotero/storage/P2R5ELVF/Comparison_of_Gaussian_process_software.html}
}

@book{rasmussen_gaussian_2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2006},
  series = {Adaptive Computation and Machine Learning},
  publisher = {MIT Press},
  address = {Cambridge, Mass},
  isbn = {978-0-262-18253-9},
  langid = {english},
  lccn = {QA274.4 .R37 2006},
  keywords = {Data processing,Gaussian processes,Machine learning,Mathematical models},
  annotation = {OCLC: ocm61285753},
  file = {/Users/CharlieKapsiak/Google Drive/Zotero/Rasmussen_Williams_2006_Gaussian processes for machine learning.pdf}
}

@misc{salimbeni_doubly_2017,
  title = {Doubly {{Stochastic Variational Inference}} for {{Deep Gaussian Processes}}},
  author = {Salimbeni, Hugh and Deisenroth, Marc},
  year = {2017},
  month = nov,
  number = {arXiv:1705.08933},
  eprint = {1705.08933},
  primaryclass = {stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1705.08933},
  urldate = {2024-05-03},
  abstract = {Gaussian processes (GPs) are a good choice for function approximation as they are flexible, robust to overfitting, and provide well-calibrated predictive uncertainty. Deep Gaussian processes (DGPs) are multi-layer generalizations of GPs, but inference in these models has proved challenging. Existing approaches to inference in DGP models assume approximate posteriors that force independence between the layers, and do not work well in practice. We present a doubly stochastic variational inference algorithm that does not force independence between layers. With our method of inference we demonstrate that a DGP model can be used effectively on data ranging in size from hundreds to a billion points. We provide strong empirical evidence that our inference scheme for DGPs works well in practice in both classification and regression.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Machine Learning},
  file = {/Users/CharlieKapsiak/Google Drive/Zotero/Salimbeni_Deisenroth_2017_Doubly Stochastic Variational Inference for Deep Gaussian Processes.pdf}
}

@article{titsias_variational_nodate,
  title = {Variational {{Learning}} of {{Inducing Variables}} in {{Sparse Gaussian Processes}}},
  author = {Titsias, Michalis K},
  abstract = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs are defined to be variational parameters which are selected by minimizing the Kullback-Leibler divergence between the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.},
  langid = {english},
  file = {/Users/CharlieKapsiak/Google Drive/Zotero/Titsias_Variational Learning of Inducing Variables in Sparse Gaussian Processes.pdf}
}

@article{wainwright_graphical_2007-1,
  title = {Graphical {{Models}}, {{Exponential Families}}, and {{Variational Inference}}},
  author = {Wainwright, Martin J. and Jordan, Michael I.},
  year = {2007},
  journal = {Foundations and Trends{\textregistered} in Machine Learning},
  volume = {1},
  number = {1--2},
  pages = {1--305},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000001},
  url = {http://www.nowpublishers.com/article/Details/MAL-001},
  urldate = {2024-03-25},
  langid = {english},
  file = {/Users/CharlieKapsiak/Google Drive/Zotero/Wainwright_Jordan_2007_Graphical Models, Exponential Families, and Variational Inference.pdf}
}

@misc{wilson_deep_2015,
  title = {Deep {{Kernel Learning}}},
  author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
  year = {2015},
  month = nov,
  number = {arXiv:1511.02222},
  eprint = {1511.02222},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/1511.02222},
  urldate = {2024-05-03},
  abstract = {We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric flexibility of kernel methods. Specifically, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation. These closed-form kernels can be used as drop-in replacements for standard kernels, with benefits in expressive power and scalability. We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process. Inference and learning cost O(n) for n training points, and predictions cost O(1) per test point. On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with flexible kernel learning models, and stand-alone deep architectures.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/CharlieKapsiak/Google Drive/Zotero/Wilson et al_2015_Deep Kernel Learning.pdf}
}

@misc{zisopoulos_parametric_2023,
  title = {Parametric Fits with Empirical Functions},
  author = {Zisopoulos, Ilias},
  year = {2023},
  month = may,
  url = {https://indico.cern.ch/event/1275872/},
  langid = {english},
  file = {/Users/CharlieKapsiak/Google Drive/Zotero/Zisopoulos_Parametric fits with empirical functions.pdf}
}
