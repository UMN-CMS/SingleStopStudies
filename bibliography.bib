
@article{evans_lhc_2013,
	title = {{LHC} {Coverage} of {RPV} {MSSM} with {Light} {Stops}},
	volume = {2013},
	issn = {1029-8479},
	url = {http://arxiv.org/abs/1209.0764},
	doi = {10.1007/JHEP04(2013)028},
	abstract = {We examine the sensitivity of recent LHC searches to signatures of supersymmetry with R-parity violation (RPV). Motivated by naturalness of the Higgs potential, which would favor light third-generation squarks, and the stringent LHC bounds on spectra in which the gluino or ﬁrst and second generation squarks are light, we focus on scenarios dominated by the pair production of light stops. We consider the various possible direct and cascade decays of the stop that involve the trilinear RPV operators. We ﬁnd that in many cases, the existing searches exclude stops in the natural mass range and beyond. However, typically there is little or no sensitivity to cases dominated by UDD operators or LQD operators involving taus. We propose several ideas for searches which could address the existing gaps in experimental coverage of these signals.},
	language = {en},
	number = {4},
	urldate = {2021-06-14},
	journal = {Journal of High Energy Physics},
	author = {Evans, Jared A. and Kats, Yevgeny},
	month = apr,
	year = {2013},
	note = {arXiv: 1209.0764},
	keywords = {High Energy Physics - Experiment, High Energy Physics - Phenomenology},
	pages = {28},
	annote = {Comment: 41 pages, 12 figures; v2: included new searches (see footnote 10), minor corrections and improvements},
}

@misc{gardner_gpytorch_2021,
	title = {{GPyTorch}: {Blackbox} {Matrix}-{Matrix} {Gaussian} {Process} {Inference} with {GPU} {Acceleration}},
	shorttitle = {{GPyTorch}},
	url = {http://arxiv.org/abs/1809.11165},
	abstract = {Despite advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on developments in computing hardware. We present an efﬁcient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modiﬁed batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. BBMM reduces the asymptotic complexity of exact GP inference from O(n3) to O(n2). Adapting this algorithm to scalable approximations and complex GP models simply requires a routine for efﬁcient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM uses a specialized preconditioner to substantially speed up convergence. In experiments we show that BBMM effectively uses GPU hardware to dramatically accelerate both exact GP inference and scalable approximations. Additionally, we provide GPyTorch, a software platform for scalable GP inference via BBMM, built on PyTorch.},
	language = {en},
	urldate = {2024-05-03},
	publisher = {arXiv},
	author = {Gardner, Jacob R. and Pleiss, Geoff and Bindel, David and Weinberger, Kilian Q. and Wilson, Andrew Gordon},
	month = jun,
	year = {2021},
	note = {arXiv:1809.11165 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NeurIPS 2018. Most recent version includes additional details on preconditioned BBMM},
	file = {Gardner et al_2021_GPyTorch.pdf:/Users/CharlieKapsiak/Google Drive/Zotero/Gardner et al_2021_GPyTorch.pdf:application/pdf},
}

@misc{salimbeni_doubly_2017,
	title = {Doubly {Stochastic} {Variational} {Inference} for {Deep} {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1705.08933},
	abstract = {Gaussian processes (GPs) are a good choice for function approximation as they are ﬂexible, robust to overﬁtting, and provide well-calibrated predictive uncertainty. Deep Gaussian processes (DGPs) are multi-layer generalizations of GPs, but inference in these models has proved challenging. Existing approaches to inference in DGP models assume approximate posteriors that force independence between the layers, and do not work well in practice. We present a doubly stochastic variational inference algorithm that does not force independence between layers. With our method of inference we demonstrate that a DGP model can be used effectively on data ranging in size from hundreds to a billion points. We provide strong empirical evidence that our inference scheme for DGPs works well in practice in both classiﬁcation and regression.},
	language = {en},
	urldate = {2024-05-03},
	publisher = {arXiv},
	author = {Salimbeni, Hugh and Deisenroth, Marc},
	month = nov,
	year = {2017},
	note = {arXiv:1705.08933 [stat]},
	keywords = {Statistics - Machine Learning},
	annote = {Comment: NIPS 2017},
	file = {Salimbeni_Deisenroth_2017_Doubly Stochastic Variational Inference for Deep Gaussian Processes.pdf:/Users/CharlieKapsiak/Google Drive/Zotero/Salimbeni_Deisenroth_2017_Doubly Stochastic Variational Inference for Deep Gaussian Processes.pdf:application/pdf},
}

@misc{wilson_deep_2015,
	title = {Deep {Kernel} {Learning}},
	url = {http://arxiv.org/abs/1511.02222},
	abstract = {We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric ﬂexibility of kernel methods. Speciﬁcally, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation. These closed-form kernels can be used as drop-in replacements for standard kernels, with beneﬁts in expressive power and scalability. We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process. Inference and learning cost O(n) for n training points, and predictions cost O(1) per test point. On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with ﬂexible kernel learning models, and stand-alone deep architectures.},
	language = {en},
	urldate = {2024-05-03},
	publisher = {arXiv},
	author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
	month = nov,
	year = {2015},
	note = {arXiv:1511.02222 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	annote = {Comment: 19 pages, 6 figures},
	file = {Wilson et al_2015_Deep Kernel Learning.pdf:/Users/CharlieKapsiak/Google Drive/Zotero/Wilson et al_2015_Deep Kernel Learning.pdf:application/pdf},
}

@book{gelman_bayesian_nodate,
	title = {Bayesian {Data} {Analysis} {Third} edition (with errors ﬁxed as of 15 {February} 2021)},
	language = {en},
	author = {Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David B and Vehtari, Aki and Rubin, Donald B},
	file = {Gelman et al_Bayesian Data Analysis Third edition (with errors ﬁxed as of 15 February 2021).pdf:/Users/CharlieKapsiak/Google Drive/Zotero/Gelman et al_Bayesian Data Analysis Third edition (with errors ﬁxed as of 15 February 2021).pdf:application/pdf},
}

@book{neal_mcmc_2011,
	title = {{MCMC} using {Hamiltonian} dynamics},
	url = {http://arxiv.org/abs/1206.1901},
	abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard-to-compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, I discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for deciding on acceptance or rejection, computing trajectories using fast approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories from taking much computation time.},
	language = {en},
	urldate = {2024-03-27},
	author = {Neal, Radford M.},
	month = may,
	year = {2011},
	doi = {10.1201/b10905},
	note = {arXiv:1206.1901 [physics, stat]},
	keywords = {Physics - Computational Physics, Statistics - Computation},
	file = {Neal_2011_MCMC using Hamiltonian dynamics.pdf:/Users/CharlieKapsiak/Google Drive/Zotero/Neal_2011_MCMC using Hamiltonian dynamics.pdf:application/pdf},
}

@article{wainwright_graphical_2007,
	title = {Graphical {Models}, {Exponential} {Families}, and {Variational} {Inference}},
	volume = {1},
	issn = {1935-8237, 1935-8245},
	url = {http://www.nowpublishers.com/article/Details/MAL-001},
	doi = {10.1561/2200000001},
	language = {en},
	number = {1–2},
	urldate = {2024-03-25},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Wainwright, Martin J. and Jordan, Michael I.},
	year = {2007},
	pages = {1--305},
	file = {Wainwright_Jordan_2007_Graphical Models, Exponential Families, and Variational Inference.pdf:/Users/CharlieKapsiak/Google Drive/Zotero/Wainwright_Jordan_2007_Graphical Models, Exponential Families, and Variational Inference.pdf:application/pdf},
}

@article{titsias_variational_nodate,
	title = {Variational {Learning} of {Inducing} {Variables} in {Sparse} {Gaussian} {Processes}},
	abstract = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs are deﬁned to be variational parameters which are selected by minimizing the Kullback-Leibler divergence between the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.},
	language = {en},
	author = {Titsias, Michalis K},
	file = {Titsias_Variational Learning of Inducing Variables in Sparse Gaussian Processes.pdf:/Users/CharlieKapsiak/Google Drive/Zotero/Titsias_Variational Learning of Inducing Variables in Sparse Gaussian Processes.pdf:application/pdf},
}

@article{gandrakota_model_2023,
	title = {Model selection and signal extraction using {Gaussian} {Process} regression},
	volume = {2023},
	issn = {1029-8479},
	url = {https://link.springer.com/10.1007/JHEP02(2023)230},
	doi = {10.1007/JHEP02(2023)230},
	abstract = {We present a novel computational approach for extracting localized signals from smooth background distributions. We focus on datasets that can be naturally presented as binned integer counts, demonstrating our procedure on the CERN open dataset with the Higgs boson signature, from the ATLAS collaboration at the Large Hadron Collider. Our approach is based on Gaussian Process (GP) regression — a powerful and ﬂexible machine learning technique which has allowed us to model the background without specifying its functional form explicitly and separately measure the background and signal contributions in a robust and reproducible manner. Unlike functional ﬁts, our GPregression-based approach does not need to be constantly updated as more data becomes available. We discuss how to select the GP kernel type, considering trade-oﬀs between kernel complexity and its ability to capture the features of the background distribution. We show that our GP framework can be used to detect the Higgs boson resonance in the data with more statistical signiﬁcance than a polynomial ﬁt speciﬁcally tailored to the dataset. Finally, we use Markov Chain Monte Carlo (MCMC) sampling to conﬁrm the statistical signiﬁcance of the extracted Higgs signature.},
	language = {en},
	number = {2},
	urldate = {2023-09-11},
	journal = {Journal of High Energy Physics},
	author = {Gandrakota, Abhijith and Lath, Amit and Morozov, Alexandre V. and Murthy, Sindhu},
	month = feb,
	year = {2023},
	pages = {230},
	file = {Gandrakota et al_2023_Model selection and signal extraction using Gaussian Process regression.pdf:/Users/CharlieKapsiak/Google Drive/Zotero/Gandrakota et al_2023_Model selection and signal extraction using Gaussian Process regression.pdf:application/pdf},
}

@misc{frate_modeling_2017,
	title = {Modeling {Smooth} {Backgrounds} and {Generic} {Localized} {Signals} with {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1709.05681},
	abstract = {We describe a procedure for constructing a model of a smooth data spectrum using Gaussian processes rather than the historical parametric description. This approach considers a fuller space of possible functions, is robust at increasing luminosity, and allows us to incorporate our understanding of the underlying physics. We demonstrate the application of this approach to modeling the background to searches for dijet resonances at the Large Hadron Collider and describe how the approach can be used in the search for generic localized signals.},
	language = {en},
	urldate = {2023-08-29},
	publisher = {arXiv},
	author = {Frate, Meghan and Cranmer, Kyle and Kalia, Saarik and Vandenberg-Rodes, Alexander and Whiteson, Daniel},
	month = sep,
	year = {2017},
	note = {arXiv:1709.05681 [hep-ex, physics:hep-ph, physics:physics]},
	keywords = {High Energy Physics - Experiment, High Energy Physics - Phenomenology, Physics - Data Analysis, Statistics and Probability},
	annote = {Comment: 14 pages, 16 figures},
	file = {Frate et al_2017_Modeling Smooth Backgrounds and Generic Localized Signals with Gaussian.pdf:/Users/CharlieKapsiak/Google Drive/Zotero/Frate et al_2017_Modeling Smooth Backgrounds and Generic Localized Signals with Gaussian.pdf:application/pdf},
}

@book{rasmussen_gaussian_2006,
	address = {Cambridge, Mass},
	series = {Adaptive computation and machine learning},
	title = {Gaussian processes for machine learning},
	isbn = {978-0-262-18253-9},
	language = {en},
	publisher = {MIT Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	year = {2006},
	note = {OCLC: ocm61285753},
	keywords = {Data processing, Gaussian processes, Machine learning, Mathematical models},
	file = {Rasmussen_Williams_2006_Gaussian processes for machine learning.pdf:/Users/CharlieKapsiak/Google Drive/Zotero/Rasmussen_Williams_2006_Gaussian processes for machine learning.pdf:application/pdf},
}

@book{hahs-vaughn_statistical_nodate,
	title = {Statistical {Concepts} - a {Second} {Course}},
	language = {en},
	author = {Hahs-Vaughn, Debbie L and Lomax, Richard G},
	file = {Hahs-Vaughn_Lomax_Statistical Concepts - a Second Course.pdf:/Users/CharlieKapsiak/Google Drive/Zotero/Hahs-Vaughn_Lomax_Statistical Concepts - a Second Course.pdf:application/pdf},
}

@misc{zisopoulos_parametric_2023,
	title = {Parametric fits with empirical functions},
	url = {https://indico.cern.ch/event/1275872/},
	language = {en},
	author = {Zisopoulos, Ilias},
	month = may,
	year = {2023},
	file = {Zisopoulos_Parametric fits with empirical functions.pdf:/Users/CharlieKapsiak/Google Drive/Zotero/Zisopoulos_Parametric fits with empirical functions.pdf:application/pdf},
}
